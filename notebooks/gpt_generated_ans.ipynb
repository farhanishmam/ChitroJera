{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API = os.getenv(\"EMNLP_OPENAI_API\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_cap(captions, question):\n",
    "    return \"\"\"\n",
    "    You are an expert Bengali Question answering assistant. Given a caption, when asked a question with the context of the caption\n",
    "    you try to provide a single worded answer by following the guidelines given below:\n",
    "    \\n \n",
    "    1:  Try to generate answer of one or two words. And the answer must never contain more than three words.\n",
    "    2:  Always answer the question in Bengali language.\n",
    "    \\n\n",
    "    CAPTION# {captions},\n",
    "    QUESTION# {question}\n",
    "    When generating the bengali answer of the question mentioned, generate in the following format:\n",
    "        \n",
    "    ANSWER# \"(generated answer)\"\n",
    "\n",
    "    \"\"\".format(captions =captions,question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_nocap(question):\n",
    "    return \"\"\"\n",
    "    You are an expert Bengali Visual Question answering assistant. Given an image, when asked a question with the context of image\n",
    "    you try to provide a single worded answer by following the guidelines given below:\n",
    "    \\n \n",
    "    1:  The answer should always be image aligned and informative.\n",
    "    2:  Try to generate answer of one or two words. And the answer must never contain more than three words.\n",
    "    3:  Always answer the question in Bengali language.\n",
    "    \\n\n",
    "    QUESTION# {question}\n",
    "    When generating the bengali answer of the question mentioned, generate in the following format:\n",
    "        \n",
    "    ANSWER# \"(generated answer)\"\n",
    "\n",
    "    \"\"\".format(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Answer(captions, question):\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API}\"\n",
    "    }\n",
    "    payload = {\n",
    "        # \"model\": \"gpt-4o\", \"gpt-4-turbo\"\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    \n",
    "                    {\"type\": \"text\", \"text\": prompt(captions, question)},\n",
    "                    # {\"type\": \"image_url\", \"image_url\": {\"url\": \n",
    "                    #     f\"data:image/jpeg;base64,{image}\"}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 30\n",
    "    }  \n",
    "    \n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    \n",
    "    return response.json() #['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"dataset/LLM_generated/valid_gpt_35_cap.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df = pd.read_csv(\"dataset/final_csvs/updated_valid.csv\")\n",
    "\n",
    "# test_df.drop(columns='GPT4o_cap',inplace=True)\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_df = pd.DataFrame(test_df[['image_name', 'Question', 'Captions', 'Answer_fixed']])\n",
    "\n",
    "cap_df.head(2)\n",
    "# cap_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_df.loc[:,'valid_GPT35_cap'] = None\n",
    "image_path = \"dataset/archive/Bangla_VQA/images\"\n",
    "# nocap_df = nocap_df.sample(frac=1)\n",
    "\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_tokens = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicedf = cap_df.iloc[0:]\n",
    "slicedf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(slicedf.iterrows(), total=slicedf.shape[0]):\n",
    "    \n",
    "    # base64_image = encode_image(f\"{image_path}/{row['image_name']}\")\n",
    "    captions = row['Captions']\n",
    "    question = row['Question']\n",
    "    \n",
    "    res = generate_Answer(captions,  question)\n",
    "    # print(response['choices'][0]['message']['content'])    \n",
    "    total_input_tokens = total_input_tokens + res['usage']['prompt_tokens']\n",
    "    total_output_tokens = total_output_tokens + res['usage']['completion_tokens']\n",
    "\n",
    "    cap_df.loc[cap_df['image_name'] == str(row['image_name']), 'valid_GPT35_cap'] = str(res['choices'][0]['message']['content'])\n",
    "    \n",
    "    print(\"done: idx: \",idx,\" filename: \", str(row['image_name']), \"input_token: \", res['usage']['prompt_tokens'], \"output_token: \", res['usage']['completion_tokens'])\n",
    "    \n",
    "    time.sleep(1)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_df.to_csv(f\"{filename}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total input tokens: \", total_input_tokens)\n",
    "print(\"total output tokens: \", total_output_tokens)\n",
    "print(f\"total cost: {total_input_tokens * 0.000005 + total_output_tokens * 0.000015}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
