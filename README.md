# ChitroJera: A Regionally Relevant Visual Question Answering Dataset for Bangla

[![anthology](https://img.shields.io/badge/-ECML%20PKDD%202025-004b96?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACEAAAAUCAYAAAADU1RxAAAACXBIWXMAAAB7AAAAewFHS0ubAAAAGXRFWHRTb2Z0d2FyZQB3d3cuaW5rc2NhcGUub3Jnm+48GgAABOBJREFUSImVlV1oHFUUx3/nzuymSZuIqUiLFttA1Yqg0lqlrslOssk2qaIgVPoiQkEffKjgB7ZiaylIX0RREdQHEcWPFxHR6GbXnS1drbSKVhDBqvWDGtumbT7cbLK7c48PmUkma1v0wMDcc+75n/853Pu/kunp+ZkmU7i/UCodBMh43naBh4DVQE3gO4Xd1phREwTF+RzXHSwUCj8C9Pf2ZrH2ZQBEbN7312az2cvt7OwhADHmvpFi8fMo10Wkq5mEiLQB9Hve0wJ7mgj2q+qTCWsTQSxXgmAI+DFc3jWPq2oBpFp1cJwuAGttWxzTzIPAoy2NRmdLo9F5bnKy6HneCmBXCPR2orX1kgasCqd0JIZRD/cMznuszQIzzc1dyNxYh6ma67oAXV1dz507dWoQkQSAdZxdw8PDk8Ak8EYTxhgwjkh3NptdGgTBFTI3hRyQPV9R4zhy3kkAdyvsV9g/MTGREGNWhv6ZYrH4+0UaSSiMAEuYne3F2s0AKlK8SM4ic2P/nwJHVbVSq9VmEzARFVm/fr1LNPbzYKhqTkR2WGMGxdqrEKmrSFlUz5ugqosm4cYi7+dLpdeiZbav7we1FsDpbG+/F3gLYGhoqKVarQYtMYylHR0HqlNTM1h7D7AM+NI0GlVMfNAXNsmk0xHdH4ATAAI786XS15l0+ntgXTiFzwRUYZMLNyq4ARxDdTpfKi0d8LwRhf4Q66lwOodRtflSydnc3b0ycJw/w/g3wBkAETkUp7oOyAAZFVmuqorr3hGSSwCbFQaBZdbaYKENEYDwXABgjRnRpsPXZDfN17L2BteIbGje4SxZcgwgn8//slfk+nI6vVFUV2HMWWvMtznfPzM0NNSiMzMbbBBYgNb29pcrlUrRVZVUsfjNFwMDrdTrG5jjyNj09Njyjo5/1WoYMy5bUqlLI8e4aqNcLk9F670i5kgqdQlAxXWnfN9vbN261ZkeHe0AsO3t00EQuInp6WSUc3O5PLEnFKjINm3a1NrpOEuinOHh4dl43My67tnoa00kJjOed3wgnd4GUM5kVkexhMgtAONjY/tC3/GgUrnc1usvxjHK6fR4xvM+7O3tvSYq0pZMPhHF69XqTL/nNfo972Amnb4dFuvEaaAisFpF3sx63rVxtqoqfX19G1X1MQCFHbkDB/6ITxY4B7QJ3OmoHhno6VkLYIyR2J4TgAVSIpLv97yUiVV50gmCtaHWOwqpJhKtJgheZ+5af1Dw/UXKKaqH877fKXArc5Ldbo15nMU2mvf9K124GjgJtAB7Fl3khjGro9OO6kQ8JiK7EbkOOG2SyQebwFERBRjx/a+Aj0NiG6MpRp0AfOL7vyq8G6besiBWIq/G7tRpt60tV6/XL4vViSYT1Gq16WYSi0zkLKog0trkX5BQ1ZPhzWlrlrQxoIC12fDBittXwN/AChceuSgJ1VXh35moPIDCPAkxZk34ezou2w/EZfvfzcnDwBZV3Qk86nneK77v/xUvnc1mlwa12p2yoJwfLcIA9TzPTRrTjeq2kNh7/03cQ7PGPAtMAcsS8HRTOGVrtb8F3mHuYB81yeTzc/3NP1hrXKhbaz9j7o35yUkmn/lfJAqFwhnghbCD7f09Peti4TpzV/SYiOxzksnbcrlcpQkiCPf8BrzkBEF3Lpc79Q83KSOPGjfUFwAAAABJRU5ErkJggg==&style=flat&labelColor=white&logoSize=auto)](https://arxiv.org/abs/2410.14991)
[![arXiv](https://img.shields.io/badge/Code-farhanishmam/ChitroJera-blue?logo=GitHub)](https://github.com/farhanishmam/ChitroJera)
[![arXiv](https://img.shields.io/badge/Dataset-sourove/ChitroJera-20BEFF?logo=Kaggle&labelColor=white&logoSize=auto)](https://www.kaggle.com/datasets/sourove/chitrojera)

[Deeparghya Dutta Barua*](https://github.com/arg274), 
[Md Sakib Ul Rahman Sourove*](https://github.com/souroveskb), 
[Md Fahim*](https://github.com/md-fahim/),
[Fabiha Haider](https://github.com/FabihaHaider),
[Fariha Tanjim Shifat](https://github.com/fariha6412), 
[Md Tasmim Rahman Adib](https://github.com/Tasmim-Adib),
[Anam Borhan Uddin](https://github.com/Borhan20),
[Md Farhan Ishmam](https://farhanishmam.github.io/), and 
[Farhad Alam Bhuiyan](https://github.com/pdfarhad).

---

ChitroJera is a Bangla regionally relevant Visual Question Answering (VQA) dataset with over 15k samples that captures the cultural connotations associated with the Bengal region. We also establish novel baselines using multimodal pre-trained models and Large Language Models (LLMs).

## Data Format

Column Title | Description
------------ | -------------
`image_id` | The unique identifier of the image
`category` | Category of the image
`caption` | Bangla caption of the image
`caption_en` | English caption of the image
`question` | Question on the image in Bangla
`question_en` | Question on the image in English
`answer` | Answer to the question in Bangla
`answer_en` | Answer to the question in English

## Data Creation Pipeline

<img src="./assets/datasetOverview.png" alt="Image Not Found"/>

The images of the ChitroJera dataset are sourced from the `BanglaLekhaCaptions`, `Bornon`, and `BNature` datasets. We establish an automated question-answer generation pipeline using the LLMs GPT-4 and Gemini. The quality of the QA pairs is checked by domain experts based on four evaluation criteria. A few images and QA pairs have been provided in the `sample_dataset` folder.

## QA Statistics

| Q&A Statistics          | Q    | A    |
|-------------------------|------|------|
| Mean character length | 33.50 | 7.10 |
| Max character length  | 105  | 45   |
| Min character length  | 11   | 1    |
| Mean word count       | 5.86 | 1.43 |
| Max word count        | 17   | 8    |
| Min word count        | 3    | 1    |


## Methodology Overview

<img src="./assets/modelOverview.PNG" alt="Image Not Found" width = "650"/>
For the baselines, we consider a dual-encoder-based architecture and the zero-shot performance of LLMs. In the dual-encoder-based model, there are two distinct training stages: pretraining and finetuning. During pretraining, both the image and text are fed into their respective encoders to obtain hidden representations, and a co-attention module is used for modality alignment. For pretraining, we use ITM, MLM, and ITC-based objectives, and during finetuning, a feature aggregation module is incorporated for classification tasks.


## Quick Start

 **Pre-trained Multimodal Model** [<img align="center" src="https://colab.research.google.com/assets/colab-badge.svg" />](https://colab.research.google.com/drive/1f6hxAPwqqis9n3i-RFB8ff5mwq_kPk-h?usp=sharing)

## Installation

We recommend using a virtual environment. Install the dependencies of this repository using:

```
pip install -r requirements.txt
```

## Training and Evaluation

To train and evaluate the model on VQA, use the following command:

```
python main.py
```
